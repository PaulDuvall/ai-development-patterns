# DEVELOPMENT_RULES - Automated Test-Driven Development

## 30-SECOND VERSION

1. **Spec ‚Üí Test ‚Üí Code ‚Üí Automate** (in that order, always)
2. **No manual commands twice** - Script it, then GitHub Action it
3. **Tests pass locally before push** - Pre-commit hook enforces
4. **Everything traces:** Spec ID ‚Üí Test ‚Üí Code ‚Üí Automation
5. **Tasks ‚â§ 3 hours** - Break larger work into atomic pieces

---

## 5-MINUTE CORE WORKFLOW

### New Feature?
```bash
# 1. Spec (2 min)
echo "FEAT-001: Login returns 200 for valid creds" > specs/FEAT-001-auth.md

# 2. Test (5 min)
cat > tests/test_auth.py << 'EOF'
def test_valid_login():
    """Implements: specs/FEAT-001-auth.md#AC-001"""
    assert login("user", "pass").status == 200
EOF
pytest tests/test_auth.py  # Should FAIL

# 3. Code (10 min)
cat > src/auth.py << 'EOF'
def login(user, pw):
    """Implements: FEAT-001#AC-001"""
    return Response(200) if valid(user, pw) else Response(401)
EOF
pytest tests/test_auth.py  # Should PASS

# 4. Automate (already done - see .github/workflows/test-and-deploy.yml)
git commit -m "feat(FEAT-001): add login" && git push
```

### Repeating Command?
```bash
# 1. Script it
echo '#!/bin/bash\npsql -c "DELETE FROM logs WHERE age > 90"' > scripts/cleanup-logs.sh
chmod +x scripts/cleanup-logs.sh

# 2. Automate it
cat > .github/workflows/cleanup-logs.yml << 'EOF'
on:
  schedule: [cron: '0 2 * * 0']
jobs:
  cleanup: {runs-on: ubuntu-latest, steps: [{run: ./scripts/cleanup-logs.sh}]}
EOF
```

---

## COMPREHENSIVE DECISION TREE

```
USER REQUEST
‚îÇ
‚îú‚îÄ NEW FEATURE/CHANGE
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Has spec? NO ‚Üí Create specs/FEAT-XXX.md ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ                  - Unique ID, acceptance criteria ‚îÇ
‚îÇ  ‚îÇ                  - Test scenarios, task breakdown ‚îÇ STOP
‚îÇ  ‚îÇ                  - Size tasks: 1-3h each          ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îú‚îÄ Has tests? NO ‚Üí Write failing tests first ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ                   - Reference spec in docstring   ‚îÇ
‚îÇ  ‚îÇ                   - Verify test fails initially   ‚îÇ STOP
‚îÇ  ‚îÇ                   - Commit: "test: FEAT-XXX"      ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îú‚îÄ Tests pass locally? NO ‚Üí Implement minimal code ‚îì
‚îÇ  ‚îÇ                            - Just enough to pass  ‚îÇ
‚îÇ  ‚îÇ                            - Add spec references  ‚îÇ STOP
‚îÇ  ‚îÇ                            - Re-run tests         ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Tests pass? YES ‚Üí Proceed to automation check
‚îÇ
‚îú‚îÄ COMMAND/SCRIPT
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ First time? ‚Üí Create script in scripts/ ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ                - Make executable                  ‚îÇ
‚îÇ  ‚îÇ                - Add --help flag                  ‚îÇ STOP
‚îÇ  ‚îÇ                - Add error handling               ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îú‚îÄ Scheduled? ‚Üí Create GitHub Action FIRST ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ               - Use cron trigger                  ‚îÇ
‚îÇ  ‚îÇ               - Add workflow_dispatch             ‚îÇ THEN RUN
‚îÇ  ‚îÇ               - Test with workflow_dispatch       ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îú‚îÄ Triggered by code? ‚Üí Add to existing workflow ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ                        - On push/PR trigger       ‚îÇ
‚îÇ  ‚îÇ                        - Runs automatically       ‚îÇ STOP
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Truly one-off? ‚Üí Document in RUNBOOK.md ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ                      - Why it was one-off            ‚îÇ
‚îÇ                      - Context for future            ‚îÇ OK
‚îÇ                                                       ‚îõ
‚îú‚îÄ BUG FIX
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Has repro test? NO ‚Üí Write failing test first ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ                       - Reproduces the bug        ‚îÇ
‚îÇ  ‚îÇ                       - Links to issue/spec       ‚îÇ STOP
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Has test? YES ‚Üí Fix & verify test passes ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ                      - Minimal fix                   ‚îÇ
‚îÇ                      - Check no regressions          ‚îÇ CONTINUE
‚îÇ                                                       ‚îõ
‚îú‚îÄ REFACTOR
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Tests exist? NO ‚Üí Write characterization tests ‚îÅ‚îì
‚îÇ  ‚îÇ                    - Document current behavior    ‚îÇ
‚îÇ  ‚îÇ                    - 100% coverage of refactor    ‚îÇ STOP
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Tests exist? YES ‚Üí Refactor & keep tests green ‚îÅ‚îì
‚îÇ                        - Tests define correctness    ‚îÇ
‚îÇ                        - No behavior changes         ‚îÇ CONTINUE
‚îÇ                                                       ‚îõ
‚îú‚îÄ INFRASTRUCTURE/CONFIG
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Manual change? ‚Üí Script it as code ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ                   - Terraform/CloudFormation      ‚îÇ
‚îÇ  ‚îÇ                   - Docker/K8s manifests          ‚îÇ STOP
‚îÇ  ‚îÇ                   - Version control it            ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Environment-specific? ‚Üí Parameterize ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ                             - Use env vars           ‚îÇ
‚îÇ                             - Separate configs       ‚îÇ CONTINUE
‚îÇ                                                       ‚îõ
‚îú‚îÄ DEPENDENCY UPDATE
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Breaking change? ‚Üí Spec + test + code workflow ‚îÅ‚îì
‚îÇ  ‚îÇ                     - Document breaking changes   ‚îÇ
‚îÇ  ‚îÇ                     - Update tests first          ‚îÇ STOP
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Non-breaking? ‚Üí Update & test ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ                     - Run full test suite            ‚îÇ
‚îÇ                     - Check for deprecations         ‚îÇ CONTINUE
‚îÇ                                                       ‚îõ
‚îú‚îÄ HOTFIX (Production Down)
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ P0 Outage? ‚Üí FIX FIRST, follow up after ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ  ‚îÇ               - Fix immediately                   ‚îÇ
‚îÇ  ‚îÇ               - Document what was done            ‚îÇ HOTFIX OK
‚îÇ  ‚îÇ               - Create issue for proper fix       ‚îÇ THEN
‚îÇ  ‚îÇ               - Follow normal workflow for issue  ‚îÇ STOP
‚îÇ  ‚îÇ                                                    ‚îõ
‚îÇ  ‚îî‚îÄ Not P0? ‚Üí Follow normal workflow ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ
‚îÇ
‚îú‚îÄ DOCUMENTATION
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ Code change? NO ‚Üí Update docs directly ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ     Code change? YES ‚Üí Update in same commit ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´ CONTINUE
‚îÇ                                                       ‚îõ
‚îî‚îÄ QUESTION/EXPLANATION
   ‚îÇ
   ‚îî‚îÄ Answer directly (no automation needed)
```

---

## WORKFLOW DETAILS

### 1. SPECIFICATION TEMPLATE

```markdown
# specs/FEAT-001-user-authentication.md
**Feature ID:** FEAT-001
**Status:** Draft ‚Üí Ready ‚Üí In Progress ‚Üí Done
**Owner:** @username
**Created:** 2025-01-15

## User Story
As a **registered user**
I want to **log in with email and password**
So that **I can access my account securely**

## Acceptance Criteria
- **AC-001:** Returns 200 + JWT for valid credentials
- **AC-002:** Returns 401 for invalid credentials
- **AC-003:** Returns 429 after 5 failed attempts in 1 minute
- **AC-004:** Logs security events (success/failure)

## Test Scenarios
| Scenario | Test Location | Status |
|----------|---------------|--------|
| Valid login | `tests/test_auth.py::test_valid_login` | ‚úÖ Pass |
| Invalid password | `tests/test_auth.py::test_invalid_password` | ‚úÖ Pass |
| Rate limiting | `tests/test_auth.py::test_rate_limit` | üöß In Progress |
| Security logging | `tests/test_auth.py::test_security_logs` | ‚ùå Not Started |

## Task Breakdown (Atomic: 1-3 hours each)
- [ ] **TASK-001** (2h): Write all test cases
- [ ] **TASK-002** (2h): Implement basic auth endpoint
- [ ] **TASK-003** (2h): Add rate limiting
- [ ] **TASK-004** (1h): Add security logging
- [ ] **TASK-005** (1h): Update API documentation

## Dependencies
- Database: Users table with hashed passwords
- External: Redis for rate limiting
- Config: JWT secret in environment

## Risks
- Rate limiting requires Redis (not in local dev by default)
- JWT secret rotation strategy needed

## Rollback Plan
- Feature flag: `FEATURE_NEW_AUTH=false`
- Old auth endpoint remains at `/auth/legacy`
- Remove after 2 weeks of monitoring
```

### 2. TEST-FIRST IMPLEMENTATION

```python
# tests/test_auth.py
"""Authentication tests
Implements: specs/FEAT-001-user-authentication.md
"""
import pytest
from src.auth import login, AuthService
from src.models import User

class TestAuthentication:
    """Test suite for FEAT-001"""

    @pytest.fixture
    def auth_service(self):
        """Implements: FEAT-001 test setup"""
        return AuthService()

    @pytest.fixture
    def valid_user(self, db):
        """Implements: FEAT-001 test data"""
        return User.create(
            email="test@example.com",
            password_hash=hash_password("ValidPass123!")
        )

    def test_valid_login(self, auth_service, valid_user):
        """Implements: specs/FEAT-001-user-authentication.md#AC-001

        Given a registered user with valid credentials
        When they attempt to login
        Then they receive a 200 response with a JWT token
        """
        response = auth_service.login(
            email="test@example.com",
            password="ValidPass123!"
        )

        assert response.status_code == 200
        assert "token" in response.json()
        assert jwt.decode(response.json()["token"]).get("user_id") == valid_user.id

    def test_invalid_password(self, auth_service, valid_user):
        """Implements: specs/FEAT-001-user-authentication.md#AC-002

        Given a registered user
        When they attempt to login with wrong password
        Then they receive a 401 response
        """
        response = auth_service.login(
            email="test@example.com",
            password="WrongPassword"
        )

        assert response.status_code == 401
        assert response.json()["error"] == "Invalid credentials"

    def test_rate_limiting(self, auth_service, valid_user):
        """Implements: specs/FEAT-001-user-authentication.md#AC-003

        Given a user has failed 5 login attempts in under 1 minute
        When they attempt to login again
        Then they receive a 429 response
        """
        # Make 5 failed attempts
        for _ in range(5):
            auth_service.login("test@example.com", "wrong")

        # 6th attempt should be rate limited
        response = auth_service.login("test@example.com", "wrong")

        assert response.status_code == 429
        assert "retry_after" in response.json()

    def test_security_logging(self, auth_service, valid_user, mock_logger):
        """Implements: specs/FEAT-001-user-authentication.md#AC-004

        Given any login attempt
        When the attempt is made
        Then a security event is logged
        """
        auth_service.login("test@example.com", "ValidPass123!")

        mock_logger.assert_called_with(
            event="login_success",
            user_id=valid_user.id,
            ip_address=ANY,
            timestamp=ANY
        )

    def test_nonexistent_user(self, auth_service):
        """Edge case: User doesn't exist

        Given an email not in the system
        When login attempted
        Then 401 returned (same as invalid password to prevent enumeration)
        """
        response = auth_service.login("nonexistent@example.com", "password")

        assert response.status_code == 401
        assert response.json()["error"] == "Invalid credentials"

# Run and verify FAILS
# $ pytest tests/test_auth.py -v
# FAILED tests/test_auth.py::TestAuthentication::test_valid_login
# FAILED tests/test_auth.py::TestAuthentication::test_invalid_password
# ...all should fail initially
```

### 3. MINIMAL IMPLEMENTATION

```python
# src/auth.py
"""Authentication service
Implements: specs/FEAT-001-user-authentication.md
"""
from datetime import datetime, timedelta
from typing import Optional
import jwt
import redis
from .models import User
from .logging import security_logger

class AuthService:
    """Implements: FEAT-001 - User Authentication

    Tests: tests/test_auth.py::TestAuthentication
    """

    def __init__(self, redis_client: redis.Redis = None):
        self.redis = redis_client or redis.from_url(settings.REDIS_URL)
        self.jwt_secret = settings.JWT_SECRET

    def login(self, email: str, password: str) -> Response:
        """Authenticate user and return JWT

        Implements:
            - FEAT-001#AC-001: Valid credentials ‚Üí 200 + JWT
            - FEAT-001#AC-002: Invalid credentials ‚Üí 401
            - FEAT-001#AC-003: Rate limiting
            - FEAT-001#AC-004: Security logging

        Tests:
            - tests/test_auth.py::test_valid_login
            - tests/test_auth.py::test_invalid_password
            - tests/test_auth.py::test_rate_limiting
            - tests/test_auth.py::test_security_logging
        """
        # AC-003: Check rate limit first
        if self._is_rate_limited(email):
            self._log_security_event("login_rate_limited", email)
            return Response(429, {"error": "Too many attempts", "retry_after": 60})

        # AC-001, AC-002: Validate credentials
        user = self._authenticate(email, password)

        if user is None:
            self._increment_failed_attempts(email)
            self._log_security_event("login_failed", email)  # AC-004
            return Response(401, {"error": "Invalid credentials"})

        # AC-001: Generate JWT
        token = self._generate_jwt(user)
        self._reset_failed_attempts(email)
        self._log_security_event("login_success", email, user.id)  # AC-004

        return Response(200, {"token": token, "user_id": user.id})

    def _authenticate(self, email: str, password: str) -> Optional[User]:
        """Verify credentials against database"""
        user = User.find_by_email(email)
        if user and user.verify_password(password):
            return user
        return None

    def _is_rate_limited(self, email: str) -> bool:
        """Implements: FEAT-001#AC-003"""
        key = f"login_attempts:{email}"
        attempts = self.redis.get(key)
        return attempts and int(attempts) >= 5

    def _increment_failed_attempts(self, email: str):
        """Implements: FEAT-001#AC-003"""
        key = f"login_attempts:{email}"
        self.redis.incr(key)
        self.redis.expire(key, 60)  # 1 minute window

    def _reset_failed_attempts(self, email: str):
        """Implements: FEAT-001#AC-003"""
        self.redis.delete(f"login_attempts:{email}")

    def _generate_jwt(self, user: User) -> str:
        """Implements: FEAT-001#AC-001"""
        payload = {
            "user_id": user.id,
            "email": user.email,
            "exp": datetime.utcnow() + timedelta(hours=24)
        }
        return jwt.encode(payload, self.jwt_secret, algorithm="HS256")

    def _log_security_event(self, event: str, email: str, user_id: int = None):
        """Implements: FEAT-001#AC-004"""
        security_logger.info({
            "event": event,
            "email": email,
            "user_id": user_id,
            "timestamp": datetime.utcnow().isoformat(),
            "ip_address": get_request_ip()
        })
```

### 4. AUTOMATION (GitHub Actions)

```yaml
# .github/workflows/test-and-deploy.yml
name: Test, Build, Deploy
on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Manual trigger

env:
  PYTHON_VERSION: '3.11'
  MIN_COVERAGE: 80

jobs:
  validate:
    name: Validate Code Quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Lint code
        run: |
          ruff check src/ tests/
          black --check src/ tests/
          mypy src/

      - name: Check traceability
        run: ./scripts/check_traceability.sh

  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: validate

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost/test
          REDIS_URL: redis://localhost:6379
        run: |
          pytest tests/unit/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=term \
            --cov-fail-under=${{ env.MIN_COVERAGE }} \
            -v

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost/test
          REDIS_URL: redis://localhost:6379
        run: |
          pytest tests/integration/ -v

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: validate
    steps:
      - uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Check for secrets
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: ${{ github.event.repository.default_branch }}
          head: HEAD

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.example.com

    steps:
      - uses: actions/checkout@v4

      - name: Deploy to staging
        run: ./scripts/deploy.sh staging
        env:
          DEPLOY_KEY: ${{ secrets.STAGING_DEPLOY_KEY }}
          DATABASE_URL: ${{ secrets.STAGING_DATABASE_URL }}

      - name: Run smoke tests
        run: ./scripts/smoke-test.sh https://staging.example.com

      - name: Notify deployment
        if: always()
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "Staging deployment ${{ job.status }}",
              "commit": "${{ github.sha }}",
              "author": "${{ github.actor }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://example.com

    steps:
      - uses: actions/checkout@v4

      - name: Create deployment
        id: deployment
        uses: chrnorm/deployment-action@v2
        with:
          token: ${{ github.token }}
          environment: production

      - name: Deploy to production
        run: ./scripts/deploy.sh production
        env:
          DEPLOY_KEY: ${{ secrets.PROD_DEPLOY_KEY }}
          DATABASE_URL: ${{ secrets.PROD_DATABASE_URL }}

      - name: Run smoke tests
        run: ./scripts/smoke-test.sh https://example.com

      - name: Update deployment status (success)
        if: success()
        uses: chrnorm/deployment-status@v2
        with:
          token: ${{ github.token }}
          deployment-id: ${{ steps.deployment.outputs.deployment_id }}
          state: 'success'

      - name: Update deployment status (failure)
        if: failure()
        uses: chrnorm/deployment-status@v2
        with:
          token: ${{ github.token }}
          deployment-id: ${{ steps.deployment.outputs.deployment_id }}
          state: 'failure'

      - name: Rollback on failure
        if: failure()
        run: ./scripts/rollback.sh production

      - name: Notify deployment
        if: always()
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "Production deployment ${{ job.status }}",
              "commit": "${{ github.sha }}",
              "author": "${{ github.actor }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

---

## FILE STRUCTURE (Complete)

```
project/
‚îÇ
‚îú‚îÄ‚îÄ specs/                              # ‚Üê Start here (Step 1)
‚îÇ   ‚îú‚îÄ‚îÄ FEAT-001-authentication.md     # Specs with IDs, ACs, test refs
‚îÇ   ‚îú‚îÄ‚îÄ FEAT-002-user-profile.md
‚îÇ   ‚îú‚îÄ‚îÄ ARCH-001-api-design.md         # Architecture decisions
‚îÇ   ‚îú‚îÄ‚îÄ traceability.md                # Spec‚ÜîTest‚ÜîCode matrix
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # Spec index and status
‚îÇ
‚îú‚îÄ‚îÄ tests/                              # ‚Üê Write second (Step 2)
‚îÇ   ‚îú‚îÄ‚îÄ unit/                          # Fast, isolated tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_auth.py              # """Implements: specs/FEAT-001#AC-001"""
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_services.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/                   # Tests with DB/external services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_database.py
‚îÇ   ‚îú‚îÄ‚îÄ e2e/                           # End-to-end tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_user_flows.py
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/                      # Test data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factories.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                    # Pytest configuration
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # How to run tests
‚îÇ
‚îú‚îÄ‚îÄ src/                                # ‚Üê Implement third (Step 3)
‚îÇ   ‚îú‚îÄ‚îÄ auth.py                        # # Implements: FEAT-001
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ
‚îú‚îÄ‚îÄ scripts/                            # ‚Üê Automate fourth (Step 4)
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                      # Deployment script
‚îÇ   ‚îú‚îÄ‚îÄ rollback.sh                    # Rollback script
‚îÇ   ‚îú‚îÄ‚îÄ smoke-test.sh                  # Post-deploy validation
‚îÇ   ‚îú‚îÄ‚îÄ cleanup-old-data.sh            # Maintenance tasks
‚îÇ   ‚îú‚îÄ‚îÄ check_traceability.sh          # Validate spec-test-code links
‚îÇ   ‚îú‚îÄ‚îÄ setup-dev-env.sh               # Developer onboarding
‚îÇ   ‚îî‚îÄ‚îÄ README.md                      # Script documentation
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îú‚îÄ‚îÄ workflows/                     # ‚Üê Final automation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test-and-deploy.yml       # Main CI/CD pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nightly-cleanup.yml       # Scheduled maintenance
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security-scan.yml         # Daily security scans
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency-update.yml     # Automated dependency PRs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Workflow documentation
‚îÇ   ‚îî‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md      # PR checklist
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/                     # Infrastructure as code
‚îÇ   ‚îú‚îÄ‚îÄ terraform/                     # Terraform configs
‚îÇ   ‚îú‚îÄ‚îÄ docker/                        # Dockerfiles
‚îÇ   ‚îî‚îÄ‚îÄ k8s/                           # Kubernetes manifests
‚îÇ
‚îú‚îÄ‚îÄ docs/                               # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ api/                           # API documentation
‚îÇ   ‚îú‚îÄ‚îÄ architecture/                  # Architecture docs
‚îÇ   ‚îú‚îÄ‚îÄ runbooks/                      # Operational procedures
‚îÇ   ‚îî‚îÄ‚îÄ onboarding/                    # New developer guides
‚îÇ
‚îú‚îÄ‚îÄ migrations/                         # Database migrations
‚îÇ   ‚îú‚îÄ‚îÄ 001_create_users.sql
‚îÇ   ‚îî‚îÄ‚îÄ 002_add_auth_indexes.sql
‚îÇ
‚îú‚îÄ‚îÄ .git/
‚îÇ   ‚îî‚îÄ‚îÄ hooks/                         # Git hooks (auto-installed)
‚îÇ       ‚îú‚îÄ‚îÄ pre-commit                 # Run tests before commit
‚îÇ       ‚îú‚îÄ‚îÄ pre-push                   # Full validation before push
‚îÇ       ‚îî‚îÄ‚îÄ commit-msg                 # Validate commit message format
‚îÇ
‚îú‚îÄ‚îÄ RUNBOOK.md                         # Manual procedures (last resort)
‚îú‚îÄ‚îÄ CHANGELOG.md                       # Auto-generated from commits
‚îú‚îÄ‚îÄ run_tests.sh                       # Local test runner (= CI)
‚îú‚îÄ‚îÄ check_traceability.sh              # Spec-test-code validation
‚îú‚îÄ‚îÄ requirements.txt                   # Production dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt               # Development dependencies
‚îú‚îÄ‚îÄ .env.example                       # Environment variable template
‚îî‚îÄ‚îÄ README.md                          # Project overview
```

---

## TRACEABILITY SYSTEM

### Bidirectional Linking

```
SPEC (specs/FEAT-001.md)
  ‚îÇ
  ‚îú‚îÄ‚Üí "Test: tests/test_auth.py::test_valid_login"
  ‚îÇ
  ‚Üì
TEST (tests/test_auth.py::test_valid_login)
  ‚îÇ
  ‚îú‚îÄ‚Üí """Implements: specs/FEAT-001.md#AC-001"""
  ‚îÇ
  ‚Üì
CODE (src/auth.py::login)
  ‚îÇ
  ‚îú‚îÄ‚Üí """Implements: FEAT-001#AC-001"""
  ‚îú‚îÄ‚Üí """Tests: tests/test_auth.py::test_valid_login"""
  ‚îÇ
  ‚Üì
WORKFLOW (.github/workflows/test-and-deploy.yml)
  ‚îÇ
  ‚îî‚îÄ‚Üí Runs tests automatically on every push
```

### Automated Validation

```bash
#!/bin/bash
# scripts/check_traceability.sh
# Validates all spec-test-code links

set -e

echo "üîç Checking traceability..."

# 1. Check specs have tests
for spec in specs/FEAT-*.md; do
    spec_id=$(basename "$spec" .md)

    # Extract test references from spec
    test_refs=$(grep -o 'tests/[^)]*' "$spec" || true)

    if [ -z "$test_refs" ]; then
        echo "‚ùå $spec_id: No test references found"
        exit 1
    fi

    # Verify each test exists
    for test_ref in $test_refs; do
        if ! grep -r "Implements.*$spec_id" tests/ > /dev/null; then
            echo "‚ùå $spec_id: Test $test_ref doesn't reference spec"
            exit 1
        fi
    done
done

# 2. Check tests have specs
for test_file in tests/test_*.py; do
    if ! grep -q "Implements: specs/" "$test_file"; then
        echo "‚ö†Ô∏è  $test_file: No spec reference found"
    fi
done

# 3. Check code has specs
for code_file in src/**/*.py; do
    if ! grep -q "Implements: FEAT-" "$code_file"; then
        echo "‚ö†Ô∏è  $code_file: No spec reference found"
    fi
done

# 4. Generate traceability matrix
python scripts/generate_traceability_matrix.py > specs/traceability.md

echo "‚úÖ Traceability check passed"
```

```python
# scripts/generate_traceability_matrix.py
"""Generate traceability matrix: Spec ‚Üî Test ‚Üî Code"""

import re
from pathlib import Path
from typing import Dict, List

def extract_spec_refs(file_path: Path) -> List[str]:
    """Extract spec references from file"""
    content = file_path.read_text()
    return re.findall(r'FEAT-\d+', content)

def extract_test_refs(file_path: Path) -> List[str]:
    """Extract test references from spec"""
    content = file_path.read_text()
    return re.findall(r'tests/[^\)]+', content)

def build_traceability_matrix():
    specs = {}

    # Scan specs
    for spec_file in Path('specs').glob('FEAT-*.md'):
        spec_id = spec_file.stem
        test_refs = extract_test_refs(spec_file)
        specs[spec_id] = {
            'tests': test_refs,
            'code': [],
            'status': '‚ùì'
        }

    # Scan tests
    for test_file in Path('tests').rglob('test_*.py'):
        content = test_file.read_text()
        for spec_id in specs:
            if spec_id in content:
                # Find implementing code
                functions = re.findall(r'def (test_\w+)', content)
                for func in functions:
                    if spec_id in content[content.find(func):content.find(func) + 500]:
                        specs[spec_id]['tests'].append(f"{test_file}::{func}")

    # Scan code
    for code_file in Path('src').rglob('*.py'):
        content = code_file.read_text()
        for spec_id in specs:
            if spec_id in content:
                specs[spec_id]['code'].append(str(code_file))
                # Check if tests pass
                if specs[spec_id]['tests']:
                    specs[spec_id]['status'] = '‚úÖ'

    # Generate markdown table
    print("# Traceability Matrix\n")
    print("| Spec ID | Tests | Implementation | Status |")
    print("|---------|-------|----------------|--------|")

    for spec_id, data in sorted(specs.items()):
        tests = '<br>'.join(data['tests'][:3]) + ('...' if len(data['tests']) > 3 else '')
        code = '<br>'.join(data['code'][:3]) + ('...' if len(data['code']) > 3 else '')
        status = data['status']
        print(f"| {spec_id} | {tests} | {code} | {status} |")

if __name__ == '__main__':
    build_traceability_matrix()
```

---

## EDGE CASES & EMERGENCIES

### Hotfix Procedure (Production Down)

```markdown
# P0 HOTFIX PROTOCOL

## Immediate Actions (Do First)
1. **Fix production immediately** - Don't wait for tests
2. **Deploy hotfix** - Use emergency deploy script
3. **Notify team** - Page on-call, post in #incidents
4. **Monitor** - Watch metrics/logs for 30 minutes

## Follow-Up Actions (Within 24 hours)
1. **Write test** - Reproduce the bug with failing test
2. **Proper fix** - Follow normal workflow (spec ‚Üí test ‚Üí code)
3. **Post-mortem** - Document in `docs/incidents/YYYY-MM-DD-incident.md`
4. **Update runbook** - Add detection/mitigation steps

## Emergency Deploy Script
```bash
# scripts/emergency-deploy.sh
#!/bin/bash
# ONLY USE FOR P0 INCIDENTS

read -p "Incident ticket #: " ticket
read -p "Confirm production deploy (yes): " confirm

if [ "$confirm" != "yes" ]; then
    echo "Cancelled"
    exit 1
fi

echo "üö® EMERGENCY DEPLOY - Incident #$ticket"
git tag -a "hotfix-$ticket" -m "Emergency hotfix for #$ticket"
./scripts/deploy.sh production --skip-tests --force
echo "üìù Create follow-up ticket to add tests"
```
```

### Legacy Code Migration

```markdown
# ADOPTING THESE RULES ON EXISTING CODEBASE

## Phase 1: Bootstrap (Week 1)
- [ ] Create `specs/` directory
- [ ] Set up GitHub Actions skeleton
- [ ] Install pre-commit hooks
- [ ] Add `run_tests.sh` script
- [ ] Create `check_traceability.sh` (lenient mode)

## Phase 2: New Code Only (Weeks 2-4)
- [ ] All NEW features follow full workflow
- [ ] Legacy code exempted (for now)
- [ ] Document legacy areas in `TECHNICAL_DEBT.md`

## Phase 3: Gradual Retrofit (Month 2+)
- [ ] When touching legacy code, add tests first
- [ ] Create spec for modified areas
- [ ] Add traceability references
- [ ] Track progress: `scripts/coverage-report.sh`

## Don't Boil the Ocean
- ‚úÖ DO: Add tests when fixing bugs in legacy code
- ‚úÖ DO: Spec new features completely
- ‚ùå DON'T: Rewrite everything at once
- ‚ùå DON'T: Block work on "perfect" setup
```

### Non-Code Changes

```markdown
# HANDLING NON-CODE CHANGES

## Documentation Changes
- Treat like code: PR ‚Üí review ‚Üí merge
- No tests needed, but CI spell-checks

## Configuration Changes
- Spec: Document what's changing and why
- Test: Validate config with smoke test
- Code: Use infrastructure-as-code (Terraform/Ansible)
- Automate: Apply via GitHub Actions

## Database Migrations
- Spec: Migration plan with rollback procedure
- Test: Run migration on test DB, verify rollback
- Code: Write both up and down migrations
- Automate: Apply in deploy script with safety checks

## Infrastructure Changes
- Spec: Architecture decision record (ADR)
- Test: Terraform plan, cost estimate
- Code: Terraform/CloudFormation
- Automate: Apply via GitHub Actions with approval gate
```

### When Tests Are Impossible

```markdown
# EXEMPTIONS (Use Sparingly)

## Legitimate Cases for No Automated Tests
1. **UI pixel-perfect alignment** - Manual QA acceptable
2. **Third-party API changes** - Contract tests + monitoring
3. **Performance tuning** - Benchmark tests instead
4. **Security patches** - Apply immediately, test after

## Required Documentation
```yaml
# specs/EXEMPT-001-ui-polish.md
exempt: true
reason: "Pixel-perfect alignment requires human judgment"
alternative_validation: "Manual QA checklist"
qa_checklist:
  - [ ] Mobile responsive
  - [ ] Dark mode works
  - [ ] Accessibility verified
manual_tester: "@designer"
```
```

---

## BOOTSTRAP SCRIPTS

### Initial Setup

```bash
#!/bin/bash
# scripts/bootstrap-project.sh
# Sets up test-first automation-first workflow from scratch

set -e

echo "üöÄ Bootstrapping test-first automation-first project..."

# 1. Create directory structure
mkdir -p specs tests/{unit,integration,e2e,fixtures} src scripts .github/workflows docs infrastructure

# 2. Create spec template
cat > specs/TEMPLATE.md << 'EOF'
# Feature: [Feature Name]
**ID:** FEAT-XXX
**Status:** Draft
**Owner:** @username

## User Story
As a [user type]
I want [feature]
So that [benefit]

## Acceptance Criteria
- AC-001: [Specific, testable requirement]

## Test Scenarios
- TEST-001: tests/test_feature.py::test_scenario

## Tasks
- [ ] TASK-001: (1-3h) [Atomic task]
EOF

# 3. Create test template
cat > tests/TEMPLATE.py << 'EOF'
"""[Feature name] tests
Implements: specs/FEAT-XXX.md
"""
import pytest

def test_example():
    """Implements: specs/FEAT-XXX.md#AC-001"""
    assert True  # Replace with actual test
EOF

# 4. Install pre-commit hooks
cat > .git/hooks/pre-commit << 'EOF'
#!/bin/bash
set -e
pytest tests/ --quiet --exitfirst || {
    echo "‚ùå Tests failed - commit blocked"
    exit 1
}
EOF
chmod +x .git/hooks/pre-commit

# 5. Create run_tests.sh (same as CI)
cat > run_tests.sh << 'EOF'
#!/bin/bash
set -e
pytest tests/ --cov=src --cov-fail-under=80 -v
./check_traceability.sh
EOF
chmod +x run_tests.sh

# 6. Create traceability checker
cat > check_traceability.sh << 'EOF'
#!/bin/bash
# Validates spec-test-code links
echo "üîç Checking traceability..."
# Add validation logic here
echo "‚úÖ Traceability check passed"
EOF
chmod +x check_traceability.sh

# 7. Create basic GitHub Action
cat > .github/workflows/test-and-deploy.yml << 'EOF'
name: Test & Deploy
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: ./run_tests.sh
EOF

# 8. Create README
cat > README.md << 'EOF'
# Project Name

## Development Workflow
1. Write spec in `specs/FEAT-XXX.md`
2. Write failing tests in `tests/`
3. Implement in `src/`
4. Push - CI auto-deploys on green

## Quick Start
```bash
./scripts/setup-dev-env.sh
./run_tests.sh
```
EOF

echo "‚úÖ Bootstrap complete!"
echo "Next steps:"
echo "  1. Create your first spec: cp specs/TEMPLATE.md specs/FEAT-001-myfeature.md"
echo "  2. Write failing tests: cp tests/TEMPLATE.py tests/test_myfeature.py"
echo "  3. Run tests: ./run_tests.sh"
```

### Developer Onboarding

```bash
#!/bin/bash
# scripts/setup-dev-env.sh
# One-command developer environment setup

set -e

echo "üëã Setting up development environment..."

# 1. Check prerequisites
command -v python3 >/dev/null 2>&1 || { echo "‚ùå Python 3 required"; exit 1; }
command -v git >/dev/null 2>&1 || { echo "‚ùå Git required"; exit 1; }

# 2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# 4. Install git hooks
cp .git/hooks/pre-commit.sample .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit

# 5. Set up local database
docker-compose up -d postgres redis

# 6. Run migrations
python scripts/migrate.py

# 7. Create .env from template
if [ ! -f .env ]; then
    cp .env.example .env
    echo "‚ö†Ô∏è  Update .env with your local settings"
fi

# 8. Run tests to verify setup
./run_tests.sh

echo "‚úÖ Development environment ready!"
echo "To activate: source .venv/bin/activate"
echo "To run tests: ./run_tests.sh"
echo "To start coding: Read CLAUDE.md for workflow"
```

---

## METRICS & MONITORING

### Track These Metrics

```yaml
# .github/workflows/metrics.yml
name: Track Metrics
on:
  push:
    branches: [main]
  schedule:
    - cron: '0 0 * * 0'  # Weekly

jobs:
  metrics:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Calculate metrics
        run: |
          echo "## Development Metrics" > metrics.md
          echo "Generated: $(date)" >> metrics.md
          echo "" >> metrics.md

          # Test coverage
          coverage=$(pytest --cov=src --cov-report=term | grep TOTAL | awk '{print $4}')
          echo "- **Test Coverage:** $coverage" >> metrics.md

          # Traceability
          specs=$(find specs -name 'FEAT-*.md' | wc -l)
          tests=$(grep -r "Implements: specs" tests | wc -l)
          echo "- **Specs:** $specs" >> metrics.md
          echo "- **Traced Tests:** $tests" >> metrics.md
          echo "- **Traceability:** $((tests * 100 / specs))%" >> metrics.md

          # Automation
          manual=$(grep -c "# TODO: Automate" scripts/*.sh || echo 0)
          automated=$(find .github/workflows -name '*.yml' | wc -l)
          echo "- **Automated Workflows:** $automated" >> metrics.md
          echo "- **Manual TODOs:** $manual" >> metrics.md

          # Task sizing
          large_tasks=$(grep -r "estimated_hours: [9-9][0-9]\|[0-9][0-9][0-9]" specs || true | wc -l)
          echo "- **Oversized Tasks:** $large_tasks" >> metrics.md

          cat metrics.md

      - name: Post to dashboard
        run: |
          # Send metrics to monitoring system
          curl -X POST ${{ secrets.METRICS_WEBHOOK }} \
            -H "Content-Type: application/json" \
            -d @metrics.md
```

---

## QUICK REFERENCE

### Common Commands

```bash
# Daily workflow
git pull                           # Get latest
./run_tests.sh                     # Run all tests (= CI)
git commit -m "feat: description"  # Commit (runs pre-commit hook)
git push                           # Push (triggers CI ‚Üí deploy)

# Creating new feature
cp specs/TEMPLATE.md specs/FEAT-042-new-thing.md
# Edit spec with acceptance criteria
cp tests/TEMPLATE.py tests/test_new_thing.py
# Write failing tests
./run_tests.sh                     # Verify tests fail
# Implement feature
./run_tests.sh                     # Verify tests pass

# Automation
./scripts/automate-command.sh "command to automate"
# Creates script + GitHub Action

# Validation
./check_traceability.sh            # Verify all links valid
pytest tests/ --cov=src            # Check coverage
```

### Commit Message Format

```
<type>(<scope>): <description>

Types: feat, fix, docs, refactor, test, chore
Scope: FEAT-XXX or component name

Examples:
  feat(FEAT-001): add user authentication
  fix(FEAT-001): handle empty password
  test(FEAT-001): add rate limiting tests
  docs: update API documentation
  chore: upgrade dependencies
```

---

## SUMMARY: The Rules

1. **Spec ‚Üí Test ‚Üí Code ‚Üí Automate** (always this order)
2. **Tests pass locally before push** (enforced by pre-commit hook)
3. **Everything is traced** (Spec ‚Üî Test ‚Üî Code ‚Üî Automation)
4. **Tasks ‚â§ 3 hours** (atomic, independent, testable)
5. **No manual commands twice** (script it, then automate it)
6. **CI = Local** (same tests, same results)
7. **Green before merge** (CI must pass)
8. **Auto-deploy on green** (staging immediately, production on tag)
9. **Hotfixes allowed** (but follow up within 24h)
10. **Gradual adoption OK** (new code first, legacy incrementally)

**Emergency Override:** Production down? Fix first, follow up after.
**Legacy Code:** Exempt until touched, then add tests.
**When Stuck:** Document in RUNBOOK.md, create issue to automate.

---

**Remember: If you do it twice, script it. If you script it, automate it. If you code it, test it first.**
